import pandas as pd

data =pd.read_csv("diabetic_data.csv",  na_values=['?', 'Unknown/Invalid', 'NA', 'N/A', 'null', 'NULL']
)
dt = data.copy()
df =pd.DataFrame(dt)

df


df.info()


df.drop(columns=['encounter_id', 'patient_nbr', 'admission_type_id' , 'discharge_disposition_id' , 'admission_source_id' ], inplace=True)
 

df.info()


missing_ratio = df.isna().mean().sort_values(ascending=False)
missing_ratio

#(df.isna().mean() * 100).sort_values(ascending=False)


high_missing_cols = missing_ratio[missing_ratio > 0.8].index
df.drop(columns=high_missing_cols, inplace=True)

df['age'] = df['age'].str.extract(r'(\d+)-').astype(float) + 5

df


obj_cols = df.select_dtypes(include='object').columns

for col in obj_cols:
    if df[col].nunique()< 10 :
        print(col, df[col].unique())

# for col in obj_cols:
    # print(col)
    # print(df[col].value_counts(dropna=False).head(5))
    # print('-'*40)



numeric_like = []
for col in obj_cols:
    s = df[col].astype(str).str.strip()
    tmp = pd.to_numeric(s, errors='coerce')
    ratio = tmp.notna().mean()
    if ratio > 0.8:  
        numeric_like.append((col, ratio))

sorted(numeric_like, key=lambda x: x[1], reverse=True)[:20]


df['diag_1_group'] = df['diag_1'].str[0]
df['diag_2_group'] = df['diag_2'].str[0]
df['diag_3_group'] = df['diag_3'].str[0]

df.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)




df.info()


from sklearn.impute import SimpleImputer

cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')
num_imputer = SimpleImputer(strategy='median')



import numpy as np

drug_cols = [
    'metformin','repaglinide','nateglinide','chlorpropamide','glimepiride',
    'acetohexamide','glipizide','glyburide','tolbutamide','pioglitazone',
    'rosiglitazone','acarbose','miglitol','troglitazone','tolazamide',
    'examide','citoglipton','insulin',
    'glyburide-metformin','glipizide-metformin',
    'glimepiride-pioglitazone','metformin-rosiglitazone',
    'metformin-pioglitazone'
]

drug_map = {
    'No': 0,
    'Steady': 1,
    'Up': 2,
    'Down': -1
}

df[drug_cols] = df[drug_cols].replace('?', np.nan)
df[drug_cols] = df[drug_cols].apply(lambda c: c.map(drug_map))
df[drug_cols] = df[drug_cols].fillna(0)



df['change'] = df['change'].map({'Yes': 1, 'No': 0})
df['diabetesMed'] = df['diabetesMed'].map({'Yes': 1, 'No': 0})


df['readmitted'] = (df['readmitted'] == '<30').astype(int)




cat_cols = [
    'race',
    'gender',
    'payer_code',
    'medical_specialty',
    'diag_1_group',
    'diag_2_group',
    'diag_3_group'
]


df_encoded = pd.get_dummies(
    df,
    columns=cat_cols,
    drop_first=True
)



df_encoded.info()


df_encoded.select_dtypes(include='object').columns



X_all = df_encoded.drop(columns=['readmitted'])
y = df_encoded['readmitted']


from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(
    X_all, y, test_size=0.2, stratify=y, random_state=42
)


from sklearn.ensemble import RandomForestClassifier

rf_100 = RandomForestClassifier(
    n_estimators=200,
    max_leaf_nodes=100,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_100.fit(X_train, y_train)
y_pred100 = rf_100.predict(X_test)



import pandas as pd

feat_imp = pd.Series(rf_100.feature_importances_, index=X_train.columns).sort_values(ascending=False)
top_features = feat_imp.head(20).index.tolist()

top_features



X_train2 = X_train[top_features]
X_test2  = X_test[top_features]

rf2 = RandomForestClassifier(
    n_estimators=200,
    max_leaf_nodes=100,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf2.fit(X_train2, y_train)

y_pred2 = rf2.predict(X_test2)


import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred100)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])

plt.figure()
disp.plot()
plt.title("Confusion Matrix")
plt.show()



import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred2)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])

plt.figure()
disp.plot()
plt.title("Confusion Matrix")
plt.show()


rf_300 = RandomForestClassifier(
    n_estimators=200,
    max_leaf_nodes=300,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf_300.fit(X_train, y_train)



import pandas as pd

feat_imp = pd.Series(rf_300.feature_importances_, index=X_train.columns).sort_values(ascending=False)
top_features2 = feat_imp.head(20).index.tolist()

top_features2


X_train2_2 = X_train[top_features]
X_test2_2  = X_test[top_features]

rf2_300 = RandomForestClassifier(
    n_estimators=200,
    max_leaf_nodes=300,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf2_300.fit(X_train2, y_train)

y_pred2_300 = rf2_300.predict(X_test2)


import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred2_300)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])

plt.figure()
disp.plot()
plt.title("Confusion Matrix")
plt.show()



import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred2_300)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])

plt.figure()
disp.plot()
plt.title("Confusion Matrix")
plt.show()


from sklearn.metrics import classification_report

y_pred_100 = rf_100.predict(X_test)
print(classification_report(y_test, y_pred_100))



from sklearn.metrics import classification_report

y_pred_300 = rf_300.predict(X_test)
print(classification_report(y_test, y_pred_300))





# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
# import matplotlib.pyplot as plt
# from xgboost import XGBClassifier

# X_all = df_encoded.drop(columns=['readmitted'])
# y = df_encoded['readmitted']


# X_train, X_test, y_train, y_test = train_test_split(
#     X_all, y, test_size=0.2, stratify=y, random_state=42
# )


# pos = (y_train == 1).sum()
# neg = (y_train == 0).sum()
# scale_pos_weight = neg / pos


# xgb_100 = XGBClassifier(
#     n_estimators=400,
#     learning_rate=0.05,
#     max_depth=6,
#     subsample=0.9,
#     colsample_bytree=0.9,
#     reg_lambda=1.0,
#     objective='binary:logistic',
#     eval_metric='logloss',
#     tree_method='hist',
#     max_leaves=100,
#     scale_pos_weight=scale_pos_weight,
#     random_state=42
# )

# xgb_100.fit(X_train, y_train)
# y_pred_xgb100 = xgb_100.predict(X_test)

# print("XGB all features - max_leaves=100")
# print(classification_report(y_test, y_pred_xgb100))

# cm = confusion_matrix(y_test, y_pred_xgb100)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
# plt.figure()
# disp.plot()
# plt.title("XGB (All) - max_leaves=100")
# plt.show()

# feat_imp = pd.Series(xgb_100.feature_importances_, index=X_train.columns).sort_values(ascending=False)
# top_features = feat_imp.head(20).index.tolist()

# X_train2 = X_train[top_features]
# X_test2  = X_test[top_features]


# xgb_top_100 = XGBClassifier(
#     n_estimators=400,
#     learning_rate=0.05,
#     max_depth=6,
#     subsample=0.9,
#     colsample_bytree=0.9,
#     reg_lambda=1.0,
#     objective='binary:logistic',
#     eval_metric='logloss',
#     tree_method='hist',
#     max_leaves=100,
#     scale_pos_weight=scale_pos_weight,
#     random_state=42
# )

# xgb_top_100.fit(X_train2, y_train)
# y_pred_xgb_top_100 = xgb_top_100.predict(X_test2)

# print("XGB top features - max_leaves=100")
# print(classification_report(y_test, y_pred_xgb_top_100))

# cm = confusion_matrix(y_test, y_pred_xgb_top_100)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
# plt.figure()
# disp.plot()
# plt.title("XGB (Top20) - max_leaves=100")
# plt.show()


# xgb_300 = XGBClassifier(
#     n_estimators=400,
#     learning_rate=0.05,
#     max_depth=6,
#     subsample=0.9,
#     colsample_bytree=0.9,
#     reg_lambda=1.0,
#     objective='binary:logistic',
#     eval_metric='logloss',
#     tree_method='hist',
#     max_leaves=300,
#     scale_pos_weight=scale_pos_weight,
#     random_state=42
# )

# xgb_300.fit(X_train, y_train)
# y_pred_xgb300 = xgb_300.predict(X_test)

# print("XGB all features - max_leaves=300")
# print(classification_report(y_test, y_pred_xgb300))

# cm = confusion_matrix(y_test, y_pred_xgb300)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
# plt.figure()
# disp.plot()
# plt.title("XGB (All) - max_leaves=300")
# plt.show()


# xgb_top_300 = XGBClassifier(
#     n_estimators=400,
#     learning_rate=0.05,
#     max_depth=6,
#     subsample=0.9,
#     colsample_bytree=0.9,
#     reg_lambda=1.0,
#     objective='binary:logistic',
#     eval_metric='logloss',
#     tree_method='hist',
#     max_leaves=300,
#     scale_pos_weight=scale_pos_weight,
#     random_state=42
# )

# xgb_top_300.fit(X_train2, y_train)
# y_pred_xgb_top_300 = xgb_top_300.predict(X_test2)

# print("XGB top features - max_leaves=300")
# print(classification_report(y_test, y_pred_xgb_top_300))

# cm = confusion_matrix(y_test, y_pred_xgb_top_300)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
# plt.figure()
# disp.plot()
# plt.title("XGB (Top20) - max_leaves=300")
# plt.show()



import sys
print(sys.executable)




